{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Import the following libraries: \n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) read data\n",
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) convert to a numpy.ndarray type\n",
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Check the shape of the data set\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Store all rows and the 0th index as the feature data: \n",
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Store all rows and index 1 as the target variable: \n",
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) Clean up the predictors.This includes removing features that are not valuable, such as timestamp and source. \n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"pafzah\",\"password\":\"worldburn432\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Panos1\",\"password\":\"najrijkom\"}}',\n",
       "       ...,\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<script src=\\\\\"http://attacker/malicious\\\\u2011script.js\\\\\"></script>\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"}}',\n",
       "       '{\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"}}'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) Tokenize our data\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Need to pad our data as each observation has a different length\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j) Create your train set to be 75% of the data and your test set to be 25%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed , Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model 1 - RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Start by creating an instance of a Sequential model\n",
    "# b) From there, add an Embedding layer\n",
    "# c) Add a SimpleRNN layer\n",
    "# d) Finally, we will add a Dense layer\n",
    "# e) Compile model using the .compile() method\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim = num_words,\n",
    "    output_dim = 32,\n",
    "    input_length = max_log_length\n",
    "    ))\n",
    "model.add(keras.layers.SimpleRNN(units = 32,activation = 'relu'))\n",
    "model.add(Dense(units = 1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# f) Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 39s 3ms/step - loss: 0.6258 - accuracy: 0.6606 - val_loss: 0.6941 - val_accuracy: 0.5110\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 37s 2ms/step - loss: 0.6526 - accuracy: 0.5728 - val_loss: 0.6302 - val_accuracy: 0.6098\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 40s 3ms/step - loss: 0.6145 - accuracy: 0.6109 - val_loss: 0.5816 - val_accuracy: 0.6223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x169733f98>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g) Use the .fit() method to fit the model on the train data.\n",
    "model.fit(X_train, y_train, epochs=3,batch_size = 128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 4s 644us/step\n",
      "test loss, test acc: [0.5785805242147808, 0.6386315822601318]\n"
     ]
    }
   ],
   "source": [
    "# h) Use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "results=model.evaluate(X_test, y_test,batch_size = 128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Model 2 - LSTM + Dropout Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Built LSTM Neural Network \n",
    "# b) Compile model using the .compile() method\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(\n",
    "    input_dim = num_words,\n",
    "    output_dim = 32,\n",
    "    input_length = max_log_length\n",
    "    ))\n",
    "model_2.add(LSTM(units = 64, recurrent_dropout = 0.5))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(units = 1,activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# c) Print the model summary\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 219s 15ms/step - loss: 0.5722 - accuracy: 0.6935 - val_loss: 0.2914 - val_accuracy: 0.9179\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 188s 12ms/step - loss: 0.3457 - accuracy: 0.8827 - val_loss: 0.3059 - val_accuracy: 0.8894\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 152s 10ms/step - loss: 0.2318 - accuracy: 0.9363 - val_loss: 0.1356 - val_accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x169748320>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Use the .fit() method to fit the model on the train data\n",
    "model_2.fit(X_train, y_train, epochs=3,batch_size = 128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 19s 3ms/step\n",
      "test loss, test acc: [0.14875343917669523, 0.9678816795349121]\n"
     ]
    }
   ],
   "source": [
    "# e) Use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "results_2=model_2.evaluate(X_test, y_test,batch_size = 128)\n",
    "print('test loss, test acc:', results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)create your RNN\n",
    "#b) Compiler Requirements: \n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(\n",
    "    input_dim = num_words,\n",
    "    output_dim = 32,\n",
    "    input_length = max_log_length\n",
    "    ))\n",
    "model_3.add(LSTM(units = 64, recurrent_dropout = 0.5,return_sequences=True))\n",
    "model_3.add(LSTM(units = 64, recurrent_dropout = 0.5))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(units = 100,activation='relu'))\n",
    "model_3.add(Dense(units = 1,activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1024, 64)          24832     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 66,473\n",
      "Trainable params: 66,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# c) Print the model summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 330s 22ms/step - loss: 0.6928 - accuracy: 0.5397 - val_loss: 0.6925 - val_accuracy: 0.5697\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 375s 25ms/step - loss: 0.6926 - accuracy: 0.5560 - val_loss: 0.6924 - val_accuracy: 0.5705\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 430s 29ms/step - loss: 0.6924 - accuracy: 0.5566 - val_loss: 0.6923 - val_accuracy: 0.5038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x157ca59e8>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d) Use the .fit() method to fit the model on the train data\n",
    "model_3.fit(X_train, y_train, epochs=3,batch_size = 128,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 49s 7ms/step\n",
      "test loss, test acc: [0.6922881944983761, 0.4887959361076355]\n"
     ]
    }
   ],
   "source": [
    "# e) Use the .evaluate() method to get the loss value & the accuracy value on the test data\n",
    "results_3=model_3.evaluate(X_test, y_test,batch_size = 128)\n",
    "print('test loss, test acc:', results_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptual Questions: \n",
    "\n",
    "###### 5) Explain the difference between the relu activation function and the sigmoid activation function.\n",
    "Sigmoid function is usually used in the last layer of the neural network for the binary classification questions.\n",
    "A sigmoid function will transform an input value into an output between 0.0 and 1.0. Any input larger than 1.0 will be transformed to 1.0, and inputs smaller than 0.0 will be transformed to 0.0. When used in a neural network, this leads to saturation around 1.0 and 0.0 and makes the midpoint quite sensitive to change. This means that when using the function to train a neural network, especially one with many layers, it becomes increasingly more difficult for the neural network to adapt and it’s weights and thus imporve performance. The sigmoid function can also cause neural networks to suffer from the vanishing gradient problem since error is backpropagated through the layers and decreases dramatically with each hidden layer.\n",
    "\n",
    "ReLU is usually used in the hidden layer.\n",
    "ReLU takes an input and directly outputs the input if positive and outputs 0 if negative. ReLU combines the benfits of a linear activation function (no vanishing gradient) while allowing for complex relationships to be modeled in the function. Unlike sigmoid, reLU is called a piecewise function, because half of the output is linear (the positive output) while the other half is nonlinear. The ReLU function is also much less computationally taxing than sigmoid.\n",
    "###### 6) Describe what one epoch actually is (epoch was a parameter used in the .fit() method).\n",
    "In terms of artificial neural networks, an epoch refers to one cycle through the full training dataset. Usually, training a neural network takes more than a few epochs. In other words, if we feed a neural network the training data for more than one epoch in different patterns, we hope for a better generalization when given a new \"unseen\" input (test data). An epoch is often mixed up with an iteration. \n",
    "Iterations is the number of batches or steps through partitioned packets of the training data, needed to complete one epoch.\n",
    "One motivation is that (especially for large but finite training sets) it gives the network a chance to see the previous data to readjust the model parameters so that the model is not biased towards the last few data points during training.  \n",
    "###### 7) Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets.\n",
    "Dropout is a Simple Way to Prevent Neural Networks from Overfitting.\n",
    "The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. Dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n",
    "\n",
    "\n",
    "###### 8) Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?\n",
    "RNNs were designed to work with sequence prediction problems. RNNs in general and LSTMs in particular have received the most success when working with sequences of words and paragraphs, generally called natural language processing.This includes both sequences of text and sequences of spoken language represented as a time series. They are also used as generative models that require a sequence output, not only with text, but on applications such as generating handwriting.\n",
    "So problems such as this homework assignment are better modeled with RNNs than CNNs.\n",
    "\n",
    "CNNs were designed to map image data to an output variable.\n",
    "\n",
    "They have proven so effective that they are the go-to method for any type of prediction problem involving image data as an input.\n",
    "The benefit of using CNNs is their ability to develop an internal representation of a two-dimensional image. This allows the model to learn position and scale in variant structures in the data, which is important when working with images.\n",
    "CNNs work well with data that has a spatial relationship.\n",
    "\n",
    "The CNN input is traditionally two-dimensional, a field or matrix, but can also be changed to be one-dimensional, allowing it to develop an internal representation of a one-dimensional sequence.\n",
    "\n",
    "This allows the CNN to be used more generally on other types of data that has a spatial relationship. For example, there is an order relationship between words in a document of text. There is an ordered relationship in the time steps of a time series.\n",
    "\n",
    "Although not specifically developed for non-image data, CNNs achieve state-of-the-art results on problems such as document classification used in sentiment analysis and related problems.\n",
    "\n",
    "###### 9) Explain what RNN problem is solved using LSTM and briefly describe how.\n",
    "LSTM is basically considered to avoid the problem of vanishing gradient in RNN. \n",
    "\n",
    "Theoretically, the information in RNN is supposed to follow for arbitrary large sequence but in practice this doesn't hold up.\n",
    "\n",
    "In a simple RNN with sigmoid or tanh neuron units, the later output nodes of the network are less sensitive to the input at time t = 1. This happens due to the vanishing gradient problem. (See the fading of the color in the figure above)\n",
    "\n",
    "An LSTM allows the preservation of gradients. The memory cell remembers the first input as long as the forget gate is open and the input gate is closed.\n",
    "The output gate provides finer control to switch the output layer on or off without altering the cell contents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
